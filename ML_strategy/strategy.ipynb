{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to be clear about what changes have to be made in order to acheive the results you want from the model.\n",
    "\n",
    "You should have a clear idea on what to do :\n",
    "- When training set is not fitting properly\n",
    "- When dev set is not fitting properly\n",
    "- When test set is not fitting properly\n",
    "- When it is not performing properly in real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single evaluation metric "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- May be we design multiple models, and we need a metric with which we can deicde which model si performing better.\n",
    "- For example, in case of classifiers, if we are using both precision & recall as metric, its difficult t decide the best performing model, as some of them might be good at precision while others at recall. Hence, we can use something called F1 score(Harmonic mean) which combines both and helps u as a single evaluation metric.\n",
    "\n",
    "- So basically you should be able to decide your single evaluation meric when you are desigining multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satificing and Optimising metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sometimes, its difficult to pack all your requirements in a single evaluation metric.\n",
    "![Alt text](evaluation_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basically, when you have multiple metrics, We decide one of them as optimising metric(as better as possible) and the others as satisficing metrics(should be atleast untill this point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/dev/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How can you setup these sets, to improve your performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make sure that, dev set and test set are not completely different distributions.\n",
    "- Choose a dev and test set to reflect data you expect to get in future and consider important to do well on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size of dev sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Its not like you always split the data into -> 60%, 20%, 20%.\n",
    "- Well it works when your data is not too huge or may be in the range of 1000-50000.\n",
    "- When you have millions of data, it is preferred to have a train set with 98% and 1% dev and 1% test.\n",
    "- Test set to be big enough to give high confidence in the overall performace of the system.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to change dev/test sets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are some cases, where the model does perform well on our metric + dev/test set but it may not perform as expected in the real world. In that case, you should reconsider changing your metric or dev/test sets to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why human level performace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ML algorithms started performing very well, that they started competing with the human level performance.\n",
    "- So we started comparing the models with human level performance, which became eventually feasible with incerased computation power.\n",
    "- i.e. knowing abt human level performance, can help u to understand how good your model is performing.\n",
    "- Will also help you to understand whether to work on decreasing bias or decreasing variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avoidable Bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The difference between train set error and human level error(bayes error) is called the avoidable bias.\n",
    "- Because train set error may not be the best metric to understand the performance, but comparing it with human level performance will give a better estimate of your performance, and gives you a idea of how much you can improve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving model performace (tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Avoiding Bias:\n",
    "    - Train Bigger model\n",
    "    - Train longer / better optimisation algos\n",
    "    - Hyperparameter search \n",
    "- Avoiding Variance:\n",
    "    - More data\n",
    "    - Regularization (L2 or dropout)\n",
    "    - Data augmentation\n",
    "    - Hyperparameters search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrying out error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Manually trying to analyse the mistakes of the model, is called error analysis.\n",
    "- This gives you an understanding on what type of examples, your model is not performing well and whether you could add some data which helps the model to capture such examples.\n",
    "- Basically, you will have a better idea on what problem you should consider working."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up incorrectly labelled examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build first system quickly, & then iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You might be having N number of ideas to imrpove, but build you model first, & then use Bias/Variance analysis or error analysis to decide on what next step to take to improve your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias & Variance with mismatched data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If training error - 1% and dev error - 10%, we usually think its because of high variance, but if teh dev and train set are from different distributions, then it might not be the case!! So, it is really important to split the train/dev sets considering their distributions.\n",
    "- So, whenever we have huge difference in train and dev errors its not always due to high variance, but may be sometimes due to data mismatch.\n",
    "- You can identify this issue, by splitting train set into train set and train_dev set again. Look at the image below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](data_mismatch.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing data mismatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Carrying out error analysis to try to understandd the difference between train and dev sets.\n",
    "- Make or collect more data in training set , so that it is similar to dev set.\n",
    "- We can use data synthesis to add more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can use the NN which was trained for some other task, and change the output layer with a new layer, which will be trained to our particular task.\n",
    "- Basically, this method is called Fine-tuning the pre trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transfer from Task A -> B makes sense when:\n",
    "    - Input type is same for A and B\n",
    "    - You have a lot of data for A than B\n",
    "    - Low level features from A should be helpful for learning B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
