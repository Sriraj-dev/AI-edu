{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building a NN , we may have lot of questions:\n",
    "- How many layers?\n",
    "- How many units in layers?\n",
    "- What activation function to use?\n",
    "- What is the learning rate?\n",
    " \n",
    "& so on ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Dev / Test set (Setting up your model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make sure that the test set has the same kind of distribution and new data when compared to train/dev sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias & Variance (Regularizing your network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Underfitting - High Bias, Overfitting - High variance\n",
    "- You can decide between these 2 by looking at train set error and dev set error.\n",
    "- **Regularization** : L1 regularization & L2 regularization + Dropout Regularization.\n",
    "    - Inverted Dropout technique\n",
    "    - In dropout techniques, keep prob is supposed to decrease\n",
    "- When we are overfitting, getting more data can help us, but getting more data may not be possible always, and so we can try **Data Augmentation**.\n",
    "- Early stopping can also be used to reduce variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up your Optimization problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing training sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basically turning out the mean to 0 (substract mean from every element) and then normalising the variance i.e X /= ((sigma) ** 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanishing / Exploding Gradients\n",
    "- In deep neural network, activations (W * X) will slowly vanish when W < I & explode when W > I. & similarly is the gradient.\n",
    "- We can partially mitigate this problem by initialising the weights carefully.\n",
    "- Weight matrix of some layer can be initialised as => np.random.rand(shape of matrix)*np.sqrt(2/n). This is in case of ReLu activation function & multiply with sqrt(1/n) in case of tanH activation function.\n",
    "- This type of initialisation is called as **Xavier's Initialisation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
