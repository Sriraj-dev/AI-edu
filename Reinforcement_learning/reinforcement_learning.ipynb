{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL is all about taking suitable actions in dynamic environment (state s) to maximise the cumulative reward.\n",
    "\n",
    "- You can also think of it as a AI-driven system to learn through trial & error using feedback machanism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applications include:\n",
    "- Controlling robots\n",
    "- Factory optimisation\n",
    "- Financial stock trading\n",
    "- Playing games "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically, for a given state , our model might decide to take action & also gets a reward for state s.By the result of taking this action, we might reach into a new state s^.\\\n",
    "**Jargon** : state(s), action(a), reward(R[s]), new state(s^)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Return in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The return in RL is the sum of the rewards that the system gets but weighted with the discount factor(gamma).\n",
    "- The more you increase gamma, the more impatient is your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Policy is like a function which takes in state(s) & decides the action(a).\n",
    "- So, the goal of RL is to find the policy which gives the maximum return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-2.png\" width = \"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-4.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MDP process basically means that, the future depends only on the current position but not on the prior states used to get to the current position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State action value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Denoted by Q.\n",
    "- Q(s,a) basically gives the return you get , if you take action a in state s and then move optimally to further states(one which give greater returns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-5.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So we can get the maximum possible return from state s by evaluating the max(Q(s,a) for all actions possible)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Helps us to compute state action value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-6.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So this braks Q(s,a) into the immediate returns(R(s)) + gamma times the optimal returns from new state(s^).\n",
    "- Equivalent to recursion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic (Random) environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In real life, machine does not actually implement the exact action you mentioned, because of various reasons. May be due to the slippery land or due to the wind etc....\n",
    "- If you take your policy and try it out 1000 to million times, you may get different returns in different cases.\n",
    "- So, what we supposed as a return till now is changed to expected return which is the average over all these cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-7.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-8.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We might have 90% chances that robot follows our path, but in other 10% it might take a different path from what is suggested by our policy, which in turn changes the expected return!!."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous state spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we have continous states instead of having some finite states as described above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In these cases we have a vector of values decribing our state, instead of just a single number(1-6) denoting the state. for example in case of a car - [x, y, angle , speedX, speedY,speedAngle]..etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning state value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-9.png\" width = \"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We use a neural network to estimate Q(s,a) (output of NN) for a given state-action pair.\n",
    "- We have to train this NN using bellman equation & multiple training examples to do a supervised learning.\n",
    "- We can get the training set by simulating the model by taking different actions in different states and record the return captured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-10.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using NN to train Q function is referred as deep Q Learning.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refinements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Improved NN archtecture**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-11.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case, we are using NN only once to calculate the Q value for all the actions instead of running NN for all the actions separately to get their Q values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **epsilon greedy policy:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-12.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- During learning, you dont really know what action to take next for a given state s and so we were randomly chosing the actions and building our training set accordingly.\n",
    "- It turns out that there is a better approach to chose the actions even while learning.\n",
    "- Refer, to the picture, we use exploration & exploitation steps to chose our actions and help us to train the model faster.\n",
    "- Initially we set the epsilon high (more exploration) and then we gradually decrease it as we need to explore more in the beginnning and later on we get to proper approximate of Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Mini batch & soft updates:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mini batch is to split the training data into multiple batches, to speed up the learning process. This method is applicable to linear & logistic regression as well.\n",
    "- By this process, the steps in gradient descent becomes faster as it does not iterate over all the training examples to take a single step in the gradient descent algortihm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-13.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In soft updates, you dont change the parameters of NN abruptly but make a small change in the direction suggested.\n",
    "- This stops RL from diverging a lot in the wrong direction.\n",
    "- So, mini batch & soft updates method makes our algorithm more reliable to use in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-14.png\" width = \"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
