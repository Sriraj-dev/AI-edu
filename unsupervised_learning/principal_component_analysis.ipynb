{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an algorithm that is used for visualisation.\\\n",
    "Basically reducing the n-dimensinal data to a 2-D or 3-D data, so that its easy to visualise the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In PCA, we try to get a axis that captures the variance of the data as high as possible.\n",
    "- If the axis gets the high variance that means the axis is able to get a lot of information from the dataset.\n",
    "- This axis is called the principal component. It turns out that the other axis is perpendicular to principal component and the 3rd axis is perpendicular to both of these axes.\n",
    "\n",
    "<img src = \"image-8.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In linear regression, you minimise the distance of the point from the line in the direction of y(output) but in PCA you treat all the axes equally and try to minimise the length of perpendiculars which in turn results in capturing more variance (& so a lot of information).\n",
    "- It might seem similar in picture that they are almost the same, but in case of multiple features, this turns to make a huge difference as in linear regression you are actually treating one axis separately (output) and measuring the lengths in the direction of that axis, in PCA you treat all the axis similarly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we are treating all the axis similarly and are not trying to predict a value from other features, this turns out to be a unsupervised algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation through code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"image-9.png\" width = \"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So this algorithm is used whenever we want to reduce the dimensions of the features, which is mostly used in visualisation but there are some other tasks which we can use (these methods were used earlier but not currently):\n",
    "    - **Data compression** : Instead of storing 50 features, we can store only 10 features.\n",
    "    - **Speeding up the training of Supervised Model** : If there are 100 features instead of 1000 features , out model can learn the patterns even more faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
