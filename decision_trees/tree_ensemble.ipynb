{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know about the decision trees and their construction. The problem with the decision trees is that they are more sensitive to the data and slight changes in the data can bring out an entire different tree which makes this algortihm not robust.\n",
    "\n",
    "So we build multiple possible decision trees and have them to vote for a particular new input to judge the output of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to build tree ensembles?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the **Sampling with replacement** method to construct the tree ensembles.\n",
    "\n",
    "- We generate new training data from the old one by picking one after another (after replacing).\n",
    "- Basically, lets you construct the new training set, which is little bit similar to original training set.\n",
    "\n",
    "There are few methods which help you to build tree ensembles:\n",
    "1. **Bagged Decision Tree**:\n",
    "- After generating \"B\" no. of training sets of data , we build decision trees on those sets , which will give us multiple decision trees to form tree ensembles.\n",
    "- Later, you can predict a new example by using the tree ensemble with a voting system.\n",
    "- It turns out that most of the times, these trees end up to be more likely similar (root partition & the nodes near to root are similar)\n",
    "- So, we come up even better algorithm to use in case of large no. of features.\n",
    "\n",
    "2. **Random Forest Algorithm**:\n",
    "- Initial procedure is same as the bagged decision tree algorithm but when building a tree , at a particular node we do not take all the features into consideration.\n",
    "- We consider a subset of N features into consideration while performing a splitting operation at a node & chose the best from that subset which would give me the best information_gain.\n",
    "- In this case we end up with multiple different trees which will give a better performace than the bagged decision tree algorithm.\n",
    "\n",
    "3. **Boosted Decision Tree(XG Boost)**:\n",
    "- When you go back to the bagged decision tree method , we are iterating through a loop in range(B) to generate trees from datasets created using Sampling with replacement.\n",
    "- We modify this a little bit, In sampling with replacement, all the examples in the dataset have the equal probability of being picked i.e. (1/m).\n",
    "- In this case, we try to give a higher probability for the examples on which the previous tree ensemble has failed to perform well. \n",
    "- In simple words, instead of blindly creating the datasets and building different trees on them, we are just rying to form a datasets which includes the examples on which the model is performing poor, which will eventaully increase the performace of the model so called XGBoost Model.\n",
    "\n",
    "XGBoost (eXtreme Gradient Boosting) is the open source library we use to implement this method.\n",
    "Here is how it goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier()\n",
    "#some weird error showing up on creating this model.\n",
    "\n",
    "# model.fit(X_train,y_train)\n",
    "# y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor()\n",
    "#some weird error showing up on creating this model.\n",
    "\n",
    "#model.fit(X_train,y_train)\n",
    "#y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
