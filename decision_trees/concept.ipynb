{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decison Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically you use decision trees in classification tasks, where the input features are having some fixed values usually like 0 or 1. Think about how can you decide between logistic regression or decision trees?\n",
    "(May be)In case of logistic regression you can also classify the tasks such as the probabilty of person has a disease.\n",
    "\n",
    "Or in simple sense, U can use decision trees when you can actually build a tree from the input features, in the case of cancer patient, you may not be able to build trees as the input has varied values like BP value, Size of clot, etc etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to decision trees, the most important decisions that you might look into are \n",
    "\n",
    "**How to split the examples or how to decide which feature to chose to split the data?**\n",
    "- It comes out that you moght chose a node which increases the purity of the data after splitting\n",
    "\n",
    "**How to decide whether or not to stop splitting / when to decide to create a leaf node?**\n",
    "- When the node is 100% of one class\n",
    "- When the node reaches a certain threshold depth\n",
    "- When the node has less than a threshold no. of examples\n",
    "- When improvements in purity score is less than a threshold.\n",
    "\n",
    "You should also keep in mind about the size of the tree, as it might become so large to compute or even it migh overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, how can we decide the purity of the sample like how do u know whether a particular sample of data is purely of one kind or is a mixture of 2 kinds & how do you measure such purity.\n",
    "\n",
    "Here comes the entropy : measure of impurity of the data. So we basically need as much less entropy as possible which signifies it is more pure of one or other kind.\n",
    "\n",
    "<img src=\"image.png\" alt=\"Entropy Function\" width=\"200\"/>\n",
    "\n",
    "It is defined as -> -p1 * log(p1) - p2 * log(p2).\n",
    "(In our case p2 = 1-p1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, coming to the first question on how to decide the node feature which splits the data comes out to be the one which maximises **information gain**.\n",
    "\n",
    "- evaluate the entropy of left subtree\n",
    "- evaluate the entropy of the right sub tree\n",
    "- calculate the weighted average of both which gives you sense of how impure the sample is.\n",
    "(lower values are better).\n",
    "- Here, you can actually substract that value from the entropy of the parent node which is called as information gain.\n",
    "\n",
    "Information gain : reduction in entropy.\n",
    "The higher the value is, more better it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we build a recursive algorithm that computes information gain at each node and chose the best one & then repeat the process in the left and right subtree until the base condition is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What would we do if the feature is having more than 2 discrete values?**\n",
    "\n",
    "In that case you can use the One-hot encoding technique in which we extrapolate the feature with k values into k features and only one of them will be mapped to 1 and the rest to 0.\n",
    "\n",
    "0 = [1,0,0]\n",
    "1 = [0,1,0]\n",
    "2 = [0,0,1]\n",
    "\n",
    "This type of one hot encoding is also very useful inc ase of neural networks as well. As these features are categorical values , you dont want to give a weightage to a particular value. for example, \n",
    "\n",
    "if we assign, \n",
    "green  = 0\n",
    "red = 1\n",
    "blue = 2\n",
    "\n",
    "and feed it to the neural network, it might somehow interpret that blue is superior than green & red which is not something we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What can we do if we have features with continous values rather than discrete values?**\n",
    "\n",
    "Try creating a feature , which has a threshold(can be determined after different trails) & you can split the data based on this threshold to get the maximum information gain possible. \n",
    "\n",
    "Compare this information gain with other features as well to decide on how to split the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we use Decision trees on regression problems?\n",
    "\n",
    "It turns out that, you can actually use decision trees for regression problems as well, but u might consider to change the building architecture of decision trees.\n",
    "\n",
    "#### How to split the data in case of regression problems\n",
    "- Calculate the variance of the left subtree & right subtree.\n",
    "- Calculate the weighted average of the variance.\n",
    "- Now, substract the weighted average from the variance at the root node.\n",
    "- You can chose the feature which reduces the variance the most.\n",
    "\n",
    "#### How can you decide the output of the tree\n",
    "- After constructing the tree , you can take the average of the (y) of all the examples at the leaf node which will give you the output of the tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Refer to the implementation of decision tree in the implementation code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperParamters in decision tree & tree ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Decision trees, important hyper parameters are:\n",
    "- max_depth\n",
    "- min_samples_split\n",
    "- splitter \n",
    "- max_features to look at for building the tree\n",
    "\n",
    "In Tree ensembles:\n",
    "- max_depth\n",
    "- min_samples_split\n",
    "- n_estimators : no. of trees \n",
    "- max_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Refer tree_ensemble.ipynb as a cont. to this topic**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to chose Decision Trees v/s neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision trees / Tree ensembles**\n",
    "- Works well on structured(tabular) data.\n",
    "- Not recommended for unstructured data(images, audios, text etc).\n",
    "- Faster to train.\n",
    "\n",
    "**Neural Networks**\n",
    "- Works well on all types of data (Structured / Unstructured data).\n",
    "- Slower to train.\n",
    "- Advantages of transfer learning(Can train the NN even with small dataset).\n",
    "- When working n multiple machine learning models, it comes out that NN are better performing.\n",
    "\n",
    "Overall, Decision trees can be competeting with neural networks in case of structured data as they are faster to train. NN might take a longet to train & then diagnoise & improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
